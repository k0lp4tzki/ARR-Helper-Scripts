#!/usr/bin/env python3
# Bulk Import Script for SONARR (could be adjusted for RADARR with small tweaks)
# - Scans a folder, infers series prefix from filenames (e.g. "Bluey.2019")
# - Lets you confirm the correct series once per prefix
# - Triggers Sonarr manualImport (Move/Copy) with quality, languages, release group
# - Cleans Activity ‚Üí Queue for the imported series (configurable)
#
# CLI flags:
#   --dry-run
#   --ids tvdb:1234,tmdb:5678   (filter lookup results)
#   --copy                      (use Copy instead of Move)
#   --map-add HOST:CONTAINER    (extra path mapping just for this run)
#   --print-languages           (dump /language and exit)
#   --no-queue-clean            (skip queue cleanup)
#   --keep-client               (when deleting queue items, do NOT remove from download client)
#   --blocklist                 (when deleting queue items, add to blocklist)
#
# Fill in API_KEY below.

import argparse, re, requests
from pathlib import Path
from time import sleep

API_URL = "http://localhost:8989/api/v3"
API_KEY = ""  # <-- put your key here
headers = {"X-Api-Key": API_KEY}

# Host ‚Üí Container mappings (so Sonarr-in-Docker can see your files)
PATH_MAP = {
    "/mnt/user/usenet/complete": "/data/complete",
    "/mnt/cache/usenet/complete": "/data/complete",
}

VIDEO_EXTS = {".mkv", ".mp4", ".avi", ".mov"}
PREFIX_RE  = re.compile(r"^(.*?)(S\d{1,2}E\d{1,2})", re.IGNORECASE)

# ---------- helpers ----------
def map_path_to_container(host_path: Path) -> str:
    s = str(host_path)
    for hp, cp in PATH_MAP.items():
        if s.startswith(hp):
            m = s.replace(hp, cp, 1)
            print(f"üîÑ Path mapping: {s} ‚Üí {m}")
            return m
    print(f"‚ö†Ô∏è No path mapping found for: {s}")
    return s

def sonarr_lookup(term, restrict_ids=None):
    r = requests.get(f"{API_URL}/series/lookup", headers=headers, params={"term": term})
    r.raise_for_status()
    res = r.json()
    if restrict_ids:
        out = []
        for x in res:
            if (f"tvdb:{x.get('tvdbId')}" in restrict_ids) or (f"tmdb:{x.get('tmdbId')}" in restrict_ids):
                out.append(x)
        return out
    return res

def get_library_series_by_ids(tvdb_id=None, tmdb_id=None):
    r = requests.get(f"{API_URL}/series", headers=headers)
    r.raise_for_status()
    for s in r.json():
        if tvdb_id and s.get("tvdbId") == tvdb_id: return s
        if tmdb_id and s.get("tmdbId") == tmdb_id: return s
    return None

def fetch_languages():
    r = requests.get(f"{API_URL}/language", headers=headers)
    r.raise_for_status()
    langs = r.json()
    # map by lowercased "name"; also allow "englishName" as a fallback key
    m = { (l.get("name") or "").lower(): l for l in langs }
    for l in langs:
        v = (l.get("englishName") or "").lower()
        if v and v not in m:
            m[v] = l
    return langs, m

def detect_language_objs(filename: str, lang_map: dict):
    n = filename.lower()
    if "german" in n or ".ger" in n or "german." in n:
        key = "german"
    elif "english" in n or ".eng" in n or "english." in n:
        key = "english"
    else:
        key = "unknown"
    # Return: a list of Language **objects** (not strings!)
    if key in lang_map:
        return [lang_map[key]]
    for k in ("unknown", "original"):
        if k in lang_map:
            return [lang_map[k]]
    # last resort (Sonarr requires a value)
    return [{"id": 0, "name": "Unknown", "nameLower": "unknown"}]

def find_episode(series_id: int, file_name: str):
    m = re.search(r"[Ss](\d{1,2})[Ee](\d{1,2})", file_name)
    if not m: return None, None, None
    season, epnum = int(m.group(1)), int(m.group(2))
    r = requests.get(f"{API_URL}/episode", headers=headers, params={"seriesId": series_id})
    r.raise_for_status()
    for ep in r.json():
        if ep["seasonNumber"] == season and ep["episodeNumber"] == epnum:
            return season, epnum, ep
    return season, epnum, None

def build_quality(file_name: str):
    q = "Unknown"; n = file_name.lower()
    if "2160p" in n or "4k" in n: q = "WEBDL-2160p"
    elif "1080p" in n:           q = "WEBDL-1080p"
    elif "720p"  in n:           q = "WEBDL-720p"
    return {"quality": {"name": q}, "revision": {"version": 1, "real": 0, "isRepack": False}}

def release_group_from_name(file_name: str):
    m = re.search(r"-([A-Za-z0-9]+)(?:\.[^.]+)?$", file_name)
    return m.group(1) if m else None

# ---------- queue maintenance ----------
def clean_queue(series_id: int | None = None,
                hint: str | None = None,
                remove_from_client: bool = True,
                blocklist: bool = False):
    """
    Remove matching entries from Sonarr's Activity ‚Üí Queue.
    - Prefer match by seriesId.
    - Fallback: if 'hint' is provided (e.g. 'Bluey.2019'), match if hint in title.
    """
    try:
        r = requests.get(f"{API_URL}/queue", headers=headers)
        r.raise_for_status()
        items = r.json()
    except Exception as e:
        print(f"‚ö†Ô∏è Could not read queue: {e}")
        return

    removed = 0
    for it in items:
        title = (it.get("title") or "").lower()
        sid   = it.get("seriesId")
        match = False

        if series_id is not None and sid == series_id:
            match = True
        elif hint:
            match = hint.lower() in title

        if match:
            qid = it["id"]
            try:
                requests.delete(
                    f"{API_URL}/queue/{qid}",
                    headers=headers,
                    params={
                        "removeFromClient": str(remove_from_client).lower(),
                        "blocklist": str(blocklist).lower()
                    }
                ).raise_for_status()
                removed += 1
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to delete queue item {qid}: {e}")

    if removed:
        print(f"üßπ Queue cleaned: removed {removed} item(s).")
    else:
        print("üßπ Queue: nothing to remove.")

# ---------- core ----------
def import_via_command(container_path: str,
                       series: dict,
                       episode: dict,
                       quality: dict,
                       import_mode: str,
                       release_group: str,
                       languages: list,
                       clean_after: bool,
                       queue_hint: str | None,
                       remove_from_client: bool,
                       blocklist: bool):
    payload = {
        "name": "manualImport",
        "importMode": import_mode,
        "files": [{
            "path": container_path,
            "seriesId": series["id"],
            "episodeIds": [episode["id"]],
            "quality": quality,
            "languages": languages,
            "releaseGroup": release_group
        }]
    }

    print(f"üöÄ POST /command ‚Üí {Path(container_path).name}")
    r = requests.post(f"{API_URL}/command", headers=headers, json=payload)
    if r.status_code == 201:
        print("‚úÖ Manual import accepted.")
        # Optional: brief wait then queue cleanup
        if clean_after:
            # Let Sonarr enqueue the import before we clean
            sleep(1.5)
            clean_queue(series_id=series["id"],
                        hint=queue_hint,
                        remove_from_client=remove_from_client,
                        blocklist=blocklist)
        return True
    else:
        print(f"‚ùå Import failed: HTTP {r.status_code}")
        try:
            print(r.text[:400])
        except Exception:
            pass
        return False

def process_file(series: dict,
                 host_file: Path,
                 import_mode: str,
                 dry_run: bool,
                 lang_map: dict,
                 queue_hint: str | None,
                 clean_after: bool,
                 remove_from_client: bool,
                 blocklist: bool):
    if host_file.suffix.lower() not in VIDEO_EXTS: return
    if "sample" in host_file.name.lower(): return

    print(f"\nüîÑ Processing {host_file.name}‚Ä¶")
    cont_path = map_path_to_container(host_file)

    season, epnum, ep = find_episode(series["id"], host_file.name)
    if not ep:
        if season and epnum:
            print(f"‚ùå Episode S{season:02d}E{epnum:02d} not found in Sonarr")
        else:
            print("‚ùå Could not read SxxEyy from filename")
        return

    print(f"‚úÖ Episode: {ep['title']} (ID: {ep['id']})")
    quality = build_quality(host_file.name)
    rg      = release_group_from_name(host_file.name)
    langs   = detect_language_objs(host_file.name, lang_map)

    if dry_run:
        print(f"üí° DRY-RUN ‚Üí path={cont_path}, epIds={[ep['id']]}, quality={quality['quality']['name']}, "
              f"langs={[l.get('name','?') for l in langs]}, rg={rg}, mode={import_mode}")
        return

    ok = import_via_command(cont_path, series, ep, quality, import_mode, rg, langs,
                            clean_after=clean_after,
                            queue_hint=queue_hint,
                            remove_from_client=remove_from_client,
                            blocklist=blocklist)
    if ok:  print(f"üéâ Import queued: {host_file.name} ‚Üí {series['title']}")
    else:   print(f"üí• Import failed: {host_file.name}")

# ---------- CLI ----------
def parse_args():
    ap = argparse.ArgumentParser(description="Sonarr CLI Manual Import (with Languages and Queue cleanup)")
    ap.add_argument("base", help="Base folder to scan")
    ap.add_argument("--dry-run", action="store_true")
    ap.add_argument("--ids", help="Filter: tvdb:1234,tmdb:5678")
    ap.add_argument("--copy", action="store_true", help="Copy instead of Move")
    ap.add_argument("--map-add", help="Add a host:container path mapping for this run")
    ap.add_argument("--print-languages", action="store_true", help="List Sonarr languages and exit")

    # Queue cleanup controls
    ap.add_argument("--no-queue-clean", action="store_true", help="Do not remove matching items from Activity ‚Üí Queue")
    ap.add_argument("--keep-client", action="store_true",
                    help="When deleting queue items, do NOT remove them from the download client")
    ap.add_argument("--blocklist", action="store_true",
                    help="When deleting queue items, add them to Sonarr's blocklist")
    return ap.parse_args()

def main():
    args = parse_args()
    base = Path(args.base)
    restrict_ids = args.ids.split(",") if args.ids else None
    import_mode  = "Copy" if args.copy else "Move"

    # queue options
    clean_after        = (not args.no_queue_clean)
    remove_from_client = (not args.keep_client)
    blocklist          = bool(args.blocklist)

    if args.map_add:
        hp, cp = args.map_add.split(":")
        PATH_MAP[hp] = cp
        print(f"‚ûï Mapping: {hp} ‚Üí {cp}")

    print(f"üìÇ Scanning {base}‚Ä¶")
    print(f"üîß API: {API_URL}")
    print("üóÇÔ∏è Mappings:")
    for h, c in PATH_MAP.items(): print(f"   {h} ‚Üí {c}")

    # API check
    requests.get(f"{API_URL}/system/status", headers=headers).raise_for_status()
    print("‚úÖ Connected to Sonarr API")

    langs, lang_map = fetch_languages()
    if args.print_languages:
        print("üåê Sonarr Languages:")
        for l in langs: print(f"   id={l.get('id')}  name={l.get('name')}")
        return

    cache = {}
    for f in sorted(base.glob("**/*")):
        if not f.is_file() or f.suffix.lower() not in VIDEO_EXTS: continue
        if "sample" in f.name.lower(): continue

        m = PREFIX_RE.match(f.name)
        if not m:
            print(f"‚ùå No prefix match for {f.name}")
            continue
        prefix = m.group(1).rstrip(" ._-")

        if prefix in cache:
            print(f"üîÅ Using cached series '{prefix}' ‚Üí {cache[prefix]['title']}")
            process_file(cache[prefix], f, import_mode, args.dry_run, lang_map,
                         queue_hint=prefix,
                         clean_after=clean_after,
                         remove_from_client=remove_from_client,
                         blocklist=blocklist)
            continue

        print(f"üîç Looking up series for prefix: '{prefix}'")
        results = sonarr_lookup(prefix, restrict_ids)
        if not results:
            print(f"‚ùå No matches for {f.name}")
            continue

        chosen = None
        for i, r in enumerate(results, 1):
            title, year = r.get("title"), r.get("year")
            tvdb_id, tmdb_id = r.get("tvdbId"), r.get("tmdbId")
            print(f"\nüé¨ Option {i}: {f.name}")
            print(f"   Series: {title} ({year})  TVDB:{tvdb_id} TMDB:{tmdb_id}")
            ans = input("Is this the correct series? (y/n/s=skip): ").strip().lower()
            if ans == "y":
                lib = get_library_series_by_ids(tvdb_id, tmdb_id)
                if not lib:
                    print("‚ùå Series is not in your Sonarr library ‚Äì please add it first.")
                    break
                cache[prefix] = lib
                chosen = lib
                break
            if ans == "s":
                print(f"‚è≠Ô∏è Skipped: {f.name}")
                break

        if chosen:
            process_file(chosen, f, import_mode, args.dry_run, lang_map,
                         queue_hint=prefix,
                         clean_after=clean_after,
                         remove_from_client=remove_from_client,
                         blocklist=blocklist)

if __name__ == "__main__":
    main()
